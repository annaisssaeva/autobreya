{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Текст для корпуса я собрала из разных текстов в интернете и какие-то предложения написала сама. Постаралась, чтобы в него входили различные слова с дефисом, фамилии на -ов и -их, аббревиатуры и сокращения, а так же достаточно нестандартные и относительно редкие слова вроде \"жахнуть\" и \"хой\", так как в определении их частей речи могут возникать ошибки или несоответствия."
      ],
      "metadata": {
        "id": "Z-n96LnwVrSm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hvSwD-fqiKd6"
      },
      "outputs": [],
      "source": [
        "korpus = '''\n",
        "На прошлой неделе Вронских уехал в Тель-Авив.\n",
        "Сказал, что номер у него очень хороший, даже диван-кровать есть.\n",
        "Съезд избрал ЦК РСДРП в составе 3 чел.: С. И. Радченко, Б. Л. Эйдельман и А. И. Кремер.\n",
        "Концерт Славы КПСС прошел на ура — он собрал 3 тыс. человек.\n",
        "Светлый лагер — простой и тонкий тип пива.\n",
        "Универсальный напиток, подходящий к любой ситуации.\n",
        "Он имеет светлый цвет от бледно-желтого до золотистого.\n",
        "Эль, как правило, более плотный, сладкий, алкогольный, часто \"мощный\" напиток с тонами фруктов.\n",
        "А вот ламбик — это бельгийское пиво самопроизвольного брожения.\n",
        "В 1926 году 22-летний Роберт Оппенгеймер учится у Патрика Блэкетта в Кавендишской лаборатории в Кембридже.\n",
        "Главным героем сериала «Во все тяжкие» является преподаватель химии Уолтер Уайт, а также его альтер-эго, именуемое «Гейзенбергом», которое медленно пробуждалось в процессе криминальной деятельности химика и, впоследствии, приобрело статус основного «Я».\n",
        "Центрального героя песни, «смешного карлика-шута», приводят во дворец, чтобы развеселить загрустившего короля, но в результате король в буквальном смысле помирает со смеху, а его подданные сходят с ума от смеха и остаются жить в шутовском «веселом царстве».\n",
        "Покойники, которых научили кричать «хой», пугают крестьян и заставляют подростка танцевать пого.\n",
        "Иванов жахнул меня по плечу, так что я аж вздрогнул.\n",
        "Эта катавасия мне порядком поднадоела, и я решил удалиться.\n",
        "На прошлой неделе Хветек участвовал в мюзикле \"Три мушкетёра\" и многих поразил своим выступлением.\n",
        "У вас еще есть возможность посетить шоу с участием этого южнокорейского артиста, но продажи билетов скоро закончатся!\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сама я разметила корпус опираясь на теги mystem, но с некоторыми упрощениями: я объединила SPRO, ADVPRO и APRO в тег PRO, так как у natasha нет разделения между разными местоимениями (зато стоит отметить, что там есть разделение между именами собственными и другими существительными, но это пришлось не учитывать, так как в mystem такого нет)."
      ],
      "metadata": {
        "id": "QdUIdy7R2I9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разметим с помощью майстем и добавим разметку в словарь"
      ],
      "metadata": {
        "id": "MGvmrWPwvYda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem"
      ],
      "metadata": {
        "id": "Tm-DTFgIiipA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mystem()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKjugBdIjSWT",
        "outputId": "b78dd374-3f15-4875-e744-83681118e5af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myst_result = m.analyze(korpus)\n",
        "mystem_d = {}\n",
        "for word in myst_result:\n",
        "    if 'analysis' in list(word.keys()):\n",
        "        mystem_d[word[\"text\"]] = word['analysis'][0]['gr'].split('=')[0].split(',')[0]"
      ],
      "metadata": {
        "id": "v8SiUQ9gjbTE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь разметим тот же текст с помощью пайморфи. К сожалению, тут придется токенизировать отдельно."
      ],
      "metadata": {
        "id": "HZPO-An2oeSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pymorphy2"
      ],
      "metadata": {
        "id": "ZI04ZKvkmWT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STdO6IQgopMe",
        "outputId": "f84a7d53-b324-4416-cc1c-378bfba4e3f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "thkBwWJCox6W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pymorphy_d = {}\n",
        "for word in word_tokenize(korpus):\n",
        "    if word not in  string.punctuation:\n",
        "        lemma = word\n",
        "        pos = morph.parse(word)[0].tag.POS\n",
        "        pymorphy_d[lemma] = pos"
      ],
      "metadata": {
        "id": "MjL_PaW3m_tV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, разметим текст с помощью natasha"
      ],
      "metadata": {
        "id": "kqlYGFv9IB6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEvpalXYHckg",
        "outputId": "9edc2d53-3ecd-4cde-89b3-5b9ee8a948f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.1)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.23.5)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=281b1780de93825be2c0412be70f934ed63c495f3bbea88b0c164497257b062b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: razdel, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "metadata": {
        "id": "oKCeEfndIU-R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Doc(korpus)"
      ],
      "metadata": {
        "id": "r1xYXMzQJZ9V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "natasha_d = {}\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "for word in doc.tokens:\n",
        "    if word.pos != \"PUNCT\":\n",
        "        natasha_d[word.text] = word.pos"
      ],
      "metadata": {
        "id": "WIwuY1VrIdtl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перенесем эти три словаря в файлы, чтобы было удобнее на них смотреть"
      ],
      "metadata": {
        "id": "FPi6ousAwCL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"pymorphy.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(pymorphy_d.keys()):\n",
        "        file.write(str(word) + \" \" + str(pymorphy_d[word]) + '\\n')\n",
        "\n",
        "with open(\"mystem.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(mystem_d.keys()):\n",
        "        file.write(word + \" \" + mystem_d[word] + '\\n')\n",
        "\n",
        "with open(\"natasha.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(natasha_d.keys()):\n",
        "        file.write(word + \" \" + natasha_d[word] + '\\n')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Ugc6QDG4wIQQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведем теги к одному виду, опираясь на тегсет майстема"
      ],
      "metadata": {
        "id": "p_oAbeUg_fNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mystem_d2 = {}\n",
        "for word in list(mystem_d.keys()):\n",
        "    if mystem_d[word] == \"ADVPRO\":\n",
        "        mystem_d2[word] = \"PRO\"\n",
        "    elif mystem_d[word] == \"APRO\":\n",
        "        mystem_d2[word] = \"PRO\"\n",
        "    elif mystem_d[word] == \"SPRO\":\n",
        "        mystem_d2[word] = \"PRO\"\n",
        "    else:\n",
        "        mystem_d2[word] = mystem_d[word]\n",
        "\n",
        "with open(\"mystem2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(mystem_d2.keys()):\n",
        "        file.write(word + \" \" + mystem_d2[word] + '\\n')"
      ],
      "metadata": {
        "id": "YB1k3z31_eZx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В пайморфи меняем теги на соответствующие в нашей разметке, разные формы глаголов обьединяем в \"V\", причастия я отнесла к прилагательным, а деепричастия к глаголам."
      ],
      "metadata": {
        "id": "eUQctAMOKC_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pymorphy_d2 = {}\n",
        "for word in list(pymorphy_d.keys()):\n",
        "    if pymorphy_d[word] == \"NOUN\":\n",
        "        pymorphy_d2[word] = \"S\"\n",
        "    elif pymorphy_d[word] == \"ADJF\":\n",
        "        pymorphy_d2[word] = \"A\"\n",
        "    elif pymorphy_d[word] == \"ADJS\":\n",
        "        pymorphy_d2[word] = \"A\"\n",
        "    elif pymorphy_d[word] == \"COMP\":\n",
        "        pymorphy_d2[word] = \"ADV\"\n",
        "    elif pymorphy_d[word] == \"VERB\":\n",
        "        pymorphy_d2[word] = \"V\"\n",
        "    elif pymorphy_d[word] == \"INFN\":\n",
        "        pymorphy_d2[word] = \"V\"\n",
        "    elif pymorphy_d[word] == \"PRTF\":\n",
        "        pymorphy_d2[word] = \"A\"\n",
        "    elif pymorphy_d[word] == \"PRTS\":\n",
        "        pymorphy_d2[word] = \"A\"\n",
        "    elif pymorphy_d[word] == \"GRND\":\n",
        "        pymorphy_d2[word] = \"V\"\n",
        "    elif pymorphy_d[word] == \"NUMR\":\n",
        "        pymorphy_d2[word] = \"NUM\"\n",
        "    elif pymorphy_d[word] == \"ADVB\":\n",
        "        pymorphy_d2[word] = \"ADV\"\n",
        "    elif pymorphy_d[word] == \"NPRO\":\n",
        "        pymorphy_d2[word] = \"PRO\"\n",
        "    elif pymorphy_d[word] == \"PRED\":\n",
        "        pymorphy_d2[word] = \"ADV\"\n",
        "    elif pymorphy_d[word] == \"COMP\":\n",
        "        pymorphy_d2[word] = \"ADV\"\n",
        "    elif pymorphy_d[word] == \"PREP\":\n",
        "        pymorphy_d2[word] = \"PR\"\n",
        "    elif pymorphy_d[word] == \"PRCL\":\n",
        "        pymorphy_d2[word] = \"PART\"\n",
        "    else:\n",
        "        pymorphy_d2[word] = str(pymorphy_d[word])\n",
        "\n",
        "with open(\"pymorphy2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(pymorphy_d2.keys()):\n",
        "        file.write(word + \" \" + str(pymorphy_d2[word]) + '\\n')"
      ],
      "metadata": {
        "id": "ghs5p2PcL0lt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "natasha_d2 = {}\n",
        "for word in list(natasha_d.keys()):\n",
        "    if natasha_d[word] == \"NOUN\":\n",
        "        natasha_d2[word] = \"S\"\n",
        "    elif natasha_d[word] == \"ADJ\":\n",
        "        natasha_d2[word] = \"A\"\n",
        "    elif natasha_d[word] == \"PROPN\":\n",
        "        natasha_d2[word] = \"S\"\n",
        "    elif natasha_d[word] == \"VERB\":\n",
        "        natasha_d2[word] = \"V\"\n",
        "    elif natasha_d[word] == \"CCONJ\":\n",
        "        natasha_d2[word] = \"CONJ\"\n",
        "    elif natasha_d[word] == \"PRON\":\n",
        "        natasha_d2[word] = \"PRO\"\n",
        "    elif natasha_d[word] == \"DET\":\n",
        "        natasha_d2[word] = \"A\"\n",
        "    elif natasha_d[word] == \"ADP\":\n",
        "        natasha_d2[word] = \"PR\"\n",
        "    else:\n",
        "        natasha_d2[word] = natasha_d[word]\n",
        "\n",
        "with open(\"natasha2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in list(natasha_d2.keys()):\n",
        "        file.write(word + \" \" + str(natasha_d2[word]) + '\\n')"
      ],
      "metadata": {
        "id": "EjIgn3oYICt6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Новую разметку я соранила в ноые файлы, чтобы можно было проверить, все ли поменялось так, как надо."
      ],
      "metadata": {
        "id": "954wFzcgwNw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем accuracy (разметку, сделанную предварительно мной, я добавила в файл gold.txt, оттуда мы достанем его прочитав файл)"
      ],
      "metadata": {
        "id": "8Q-UiKphd3Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_d = {}\n",
        "with open(\"gold.txt\", \"r\") as file:\n",
        "    for line in file.readlines():\n",
        "        gold_d[line.split()[0]] = line.split()[1]"
      ],
      "metadata": {
        "id": "SRatxAkHd7tr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_num = 233\n",
        "mystem_right = 0\n",
        "pymorphy_right = 0\n",
        "natasha_right = 0\n",
        "\n",
        "for word in list(gold_d.keys()):\n",
        "    if word in list(mystem_d2.keys()):\n",
        "        if mystem_d2[word] == gold_d[word]:\n",
        "            mystem_right += 1\n",
        "\n",
        "for word in list(gold_d.keys()):\n",
        "    if str(word) in list(pymorphy_d2.keys()):\n",
        "        if pymorphy_d2[word] == gold_d[word]:\n",
        "            pymorphy_right += 1\n",
        "\n",
        "for word in list(gold_d.keys()):\n",
        "    if word in list(natasha_d2.keys()):\n",
        "        if natasha_d2[word] == gold_d[word]:\n",
        "            natasha_right += 1\n",
        "\n",
        "print(f\"mystem accuracy: {mystem_right/gold_num}\")\n",
        "print(f\"pymorphy accuracy: {pymorphy_right/gold_num}\")\n",
        "print(f\"natasha accuracy: {natasha_right/gold_num}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EGtiOI0hsBE",
        "outputId": "7bdf9b90-e435-4b24-b9ef-465f4c5a6d0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mystem accuracy: 0.776824034334764\n",
            "pymorphy accuracy: 0.7553648068669528\n",
            "natasha accuracy: 0.7510729613733905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Экьюраси получилась самой большой для майстем."
      ],
      "metadata": {
        "id": "Cc0yYZQ5wwmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Думаю, в прошлой домашке в словаре пригодились бы сочетания не + прилагательное и слишком + прилагательное. Наличие их в словаре помогло бы увеличить количество слов, характерных для отрицательных отзывов, так как, кажется, что они бы были более характерны для плохой оценки чего-то.\n",
        "\n",
        "Для общего пополнения словаря же могли бы пригодиться сочетания прилагательное + существительное, так как разных персонажей, например, в отзывах могли бы называть по-разному.\n",
        "\n"
      ],
      "metadata": {
        "id": "E9zGnmAZlvcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чанкер я написала для игрушечного примера"
      ],
      "metadata": {
        "id": "Q_37yphTkAfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Это просто текcт для проверки, можно сказать проверочный текст.\n",
        "Не хочу делать его слишком длинным, поэтому не буду много писать.\n",
        "Он получается не интересным, зато поможет определить, не ошибается ли написанная программа.\n",
        "А то я в слишком волнительном настроении по этому поводу.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w_Nc7PYFkFP0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "n_gramms = []\n",
        "result = m.analyze(text)\n",
        "\n",
        "for word in result:\n",
        "    ind = result.index(word)\n",
        "    prev_ind = ind - 2\n",
        "    if result[prev_ind]['text'] in string.punctuation:\n",
        "        prev_ind = ind - 3\n",
        "    prev =  result[prev_ind]\n",
        "\n",
        "    if 'analysis' in list(word.keys()):\n",
        "        pos = word['analysis'][0]['gr'].split('=')[0].split(',')[0]\n",
        "        token = word['text']\n",
        "\n",
        "        if 'analysis' in list(prev.keys()):\n",
        "            pos_prev = prev['analysis'][0]['gr'].split('=')[0].split(',')[0]\n",
        "            word_prev = prev['text']\n",
        "\n",
        "            #достаем сочетания не + прилагательное\n",
        "            if str(word_prev).lower() == \"не\":\n",
        "                if str(pos) == \"A\":\n",
        "                    n_gramms.append(f'{word_prev} {str(token)}')\n",
        "\n",
        "            #достаем сочетания слишком + прилагательное\n",
        "            if str(word_prev).lower() == \"слишком\":\n",
        "                if str(pos) == \"A\":\n",
        "                    n_gramms.append(f'{word_prev} {str(token)}')\n",
        "\n",
        "            #достаем сочетания прилагательное + существительное\n",
        "            if str(pos_prev) == \"A\":\n",
        "                if str(pos) == \"S\":\n",
        "                    n_gramms.append(f'{word_prev} {str(token)}')\n",
        "\n",
        "\n",
        "print(n_gramms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsVbBkj75hS6",
        "outputId": "49d2f2e4-4fbe-4151-8298-599845eefad4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['проверочный текст', 'слишком длинным', 'не интересным', 'слишком волнительном', 'волнительном настроении']\n"
          ]
        }
      ]
    }
  ]
}